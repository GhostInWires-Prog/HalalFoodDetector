{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Halal Product Intelligence Pipeline\n",
        "\n",
        "> Optimised Colab-ready workflow for training halal logo detection, ingredient text classification, and unified inference with OCR + barcode support.\n",
        "\n",
        "This notebook consolidates the full workflow into modular, fault-tolerant cells so it can be executed from top to bottom in Google Colab without manual patching. Each section has guard clauses to handle missing datasets or API credentials gracefully.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Runtime Setup\n",
        "\n",
        "- âœ… Target platform: Google Colab (Python 3.10+, CUDA optional)\n",
        "- ðŸ“¦ Core dependencies: `tensorflow`, `torch`, `opencv-python`, `easyocr`, `beautifulsoup4`, `requests`, `scikit-learn`, `pyzbar`, `pandas`, `numpy`\n",
        "- ðŸ’¾ Storage: this notebook writes datasets/models to `/content/halal_pipeline`\n",
        "\n",
        "Run the cell below once after opening the notebook. It installs all requirements and performs lightweight sanity checks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"tensorflow>=2.16.0\",\n",
        "    \"torch>=2.3.0\",\n",
        "    \"torchvision>=0.18.0\",\n",
        "    \"opencv-python>=4.9.0\",\n",
        "    \"easyocr>=1.7.1\",\n",
        "    \"beautifulsoup4\",\n",
        "    \"requests\",\n",
        "    \"scikit-learn\",\n",
        "    \"pyzbar\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\",\n",
        "    \"joblib\",\n",
        "    \"roboflow\",\n",
        "]\n",
        "\n",
        "for pkg in REQUIRED_PACKAGES:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "print(\"All packages installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import base64\n",
        "import random\n",
        "import string\n",
        "import zipfile\n",
        "import logging\n",
        "import re\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "import easyocr\n",
        "from pyzbar import pyzbar\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from joblib import dump, load\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"/content/halal_pipeline\")\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
        "\n",
        "for folder in [BASE_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, OUTPUTS_DIR]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Working directory tree initialised at\", BASE_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Acquisition\n",
        "\n",
        "We collect three complementary resources:\n",
        "\n",
        "1. **E-code registry** for ingredient-level supervision (halal / haram / mushbooh).\n",
        "2. **Halal logo images** to train a binary detector (`logo` vs `no_logo`).\n",
        "3. **Optional local imagery** (ingredient panels / packaging) that you may mount manually for OCR benchmarking.\n",
        "\n",
        "Each downloader uses polite scraping with retry/backoff and skips execution when files already exist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_ecode_sources(output_csv: Path) -> pd.DataFrame:\n",
        "    if output_csv.exists():\n",
        "        logging.info(\"E-code CSV already exists: %s\", output_csv)\n",
        "        return pd.read_csv(output_csv)\n",
        "\n",
        "    sources = [\n",
        "        \"https://www.ecodehalalcheck.com/\",\n",
        "        \"https://international-halal.com/ecodes/\",\n",
        "        \"https://ecode.figlab.io/quick_list.html\",\n",
        "    ]\n",
        "\n",
        "    records: List[Dict[str, str]] = []\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    def normalise_status(text: str) -> str:\n",
        "        text = (text or \"\").strip().lower()\n",
        "        if \"haram\" in text:\n",
        "            return \"Haram\"\n",
        "        if \"mushbooh\" in text or \"doubt\" in text:\n",
        "            return \"Mushbooh\"\n",
        "        if \"halal\" in text:\n",
        "            return \"Halal\"\n",
        "        return \"Unknown\"\n",
        "\n",
        "    for url in sources:\n",
        "        logging.info(\"Scraping %s\", url)\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=20)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            tables = soup.find_all(\"table\")\n",
        "            for table in tables:\n",
        "                headers_row = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
        "                if not headers_row:\n",
        "                    continue\n",
        "\n",
        "                rows = table.find_all(\"tr\")\n",
        "                for row in rows[1:]:\n",
        "                    cells = [td.get_text(strip=True) for td in row.find_all(\"td\")]\n",
        "                    if len(cells) != len(headers_row):\n",
        "                        continue\n",
        "                    row_dict = dict(zip(headers_row, cells))\n",
        "                    code = row_dict.get(\"E-Code\") or row_dict.get(\"Code\") or row_dict.get(\"Number\")\n",
        "                    name = row_dict.get(\"Name\") or row_dict.get(\"Ingredient\") or \"\"\n",
        "                    status = row_dict.get(\"Halal Status\") or row_dict.get(\"Status\") or \"Unknown\"\n",
        "                    description = row_dict.get(\"Description\") or row_dict.get(\"Details\") or \"\"\n",
        "                    if code:\n",
        "                        records.append(\n",
        "                            {\n",
        "                                \"e_code_number\": code.strip().upper().replace(\" \", \"\"),\n",
        "                                \"name\": name,\n",
        "                                \"halal_status\": normalise_status(status),\n",
        "                                \"description\": description,\n",
        "                                \"source\": url,\n",
        "                            }\n",
        "                        )\n",
        "        except Exception as exc:\n",
        "            logging.warning(\"Failed to scrape %s: %s\", url, exc)\n",
        "\n",
        "    df = pd.DataFrame.from_records(records).drop_duplicates(\"e_code_number\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Failed to collect E-code data; check source availability\")\n",
        "\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    logging.info(\"Saved %d E-codes to %s\", len(df), output_csv)\n",
        "    return df\n",
        "\n",
        "ECODE_CSV = RAW_DIR / \"ecode_database.csv\"\n",
        "ecode_df = scrape_ecode_sources(ECODE_CSV)\n",
        "ecodedf_preview = ecode_df.head()\n",
        "print(\"Sample E-codes:\\n\", ecodedf_preview)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Optional: Download image datasets from Kaggle\n",
        "\n",
        "Skip this cell if you already placed image folders under `RAW_DIR`. To enable Kaggle downloads in Colab, upload your `kaggle.json` API token to `/content` before executing the next cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_REMOTE_DATA = True  # flip to False to rely on pre-mounted datasets only\n",
        "\n",
        "ROBOFLOW_CONFIG = {\n",
        "    \"api_key\": \"feukKanicdiSX7SHcWDn\",\n",
        "    \"workspace\": \"bugboisdd\",\n",
        "    \"project\": \"halal-logo-dqkxm-ee1v8\",\n",
        "    \"version\": 1,\n",
        "    \"format\": \"multiclass\",\n",
        "    \"target_dir\": RAW_DIR / \"roboflow_halal_logo\",\n",
        "}\n",
        "\n",
        "KAGGLE_DATASETS = {\n",
        "    \"food_ingredients_halal_label\": \"irfanakbarihabibi/food-ingredients-dataset-with-halal-label\",\n",
        "}\n",
        "\n",
        "ingredient_csv_paths: List[Path] = []\n",
        "roboflow_logo_root: Optional[Path] = None\n",
        "\n",
        "\n",
        "def download_kaggle_dataset(dataset_ref: str, target_dir: Path) -> bool:\n",
        "    cmd = [\n",
        "        \"kaggle\",\n",
        "        \"datasets\",\n",
        "        \"download\",\n",
        "        \"-d\",\n",
        "        dataset_ref,\n",
        "        \"-p\",\n",
        "        str(target_dir),\n",
        "        \"--force\",\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        logging.warning(\"Kaggle download failed for %s: %s\", dataset_ref, result.stderr.strip())\n",
        "        return False\n",
        "    for zip_path in target_dir.glob(\"*.zip\"):\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(target_dir)\n",
        "            zip_path.unlink()\n",
        "        except Exception as exc:\n",
        "            logging.warning(\"Failed to extract %s: %s\", zip_path, exc)\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download_roboflow_dataset(config: Dict[str, Any]) -> Optional[Path]:\n",
        "    try:\n",
        "        logging.info(\n",
        "            \"Downloading Roboflow dataset %s/%s (v%s)\",\n",
        "            config[\"workspace\"],\n",
        "            config[\"project\"],\n",
        "            config[\"version\"],\n",
        "        )\n",
        "        rf = Roboflow(api_key=config[\"api_key\"])\n",
        "        project = rf.workspace(config[\"workspace\"]).project(config[\"project\"])\n",
        "        version = project.version(config[\"version\"])\n",
        "        dataset = version.download(config[\"format\"])\n",
        "    except Exception as exc:\n",
        "        logging.warning(\"Roboflow download failed: %s\", exc)\n",
        "        return None\n",
        "\n",
        "    source_root = Path(getattr(dataset, \"location\", \"\"))\n",
        "    if not source_root.exists():\n",
        "        logging.warning(\"Roboflow download reported %s but the path does not exist\", source_root)\n",
        "        return None\n",
        "\n",
        "    target_dir = Path(config[\"target_dir\"])\n",
        "    if source_root.resolve() != target_dir.resolve():\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copytree(source_root, target_dir, dirs_exist_ok=True)\n",
        "        logging.info(\"Copied Roboflow dataset into %s\", target_dir)\n",
        "        return target_dir\n",
        "\n",
        "    logging.info(\"Roboflow dataset ready at %s\", source_root)\n",
        "    return source_root\n",
        "\n",
        "\n",
        "if USE_REMOTE_DATA:\n",
        "    kaggle_token = Path(\"/content/kaggle.json\")\n",
        "    if kaggle_token.exists():\n",
        "        os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content\"\n",
        "        for folder_name, dataset_ref in KAGGLE_DATASETS.items():\n",
        "            target_dir = RAW_DIR / folder_name\n",
        "            target_dir.mkdir(parents=True, exist_ok=True)\n",
        "            logging.info(\"Downloading %s to %s\", dataset_ref, target_dir)\n",
        "            success = download_kaggle_dataset(dataset_ref, target_dir)\n",
        "            if not success:\n",
        "                logging.warning(\"Skipping dataset %s due to download issues.\", dataset_ref)\n",
        "    else:\n",
        "        logging.info(\"kaggle.json not provided; skipping Kaggle dataset downloads.\")\n",
        "\n",
        "    roboflow_logo_root = download_roboflow_dataset(ROBOFLOW_CONFIG)\n",
        "else:\n",
        "    logging.info(\"Skipping remote downloads. Datasets must already be available under %s\", RAW_DIR)\n",
        "    default_logo_root = ROBOFLOW_CONFIG[\"target_dir\"]\n",
        "    if default_logo_root.exists():\n",
        "        roboflow_logo_root = default_logo_root\n",
        "\n",
        "ingredient_dataset_dir = RAW_DIR / \"food_ingredients_halal_label\"\n",
        "if ingredient_dataset_dir.exists():\n",
        "    ingredient_csv_paths = sorted(ingredient_dataset_dir.rglob(\"*.csv\"))\n",
        "else:\n",
        "    ingredient_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ingredient_csv_paths = []\n",
        "\n",
        "ROBOFLOW_LOGO_ROOT = roboflow_logo_root\n",
        "INGREDIENT_CSV_PATHS = ingredient_csv_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scan_image_directory(root: Path, label_map: Optional[Dict[str, str]] = None) -> pd.DataFrame:\n",
        "    records = []\n",
        "    if not root.exists():\n",
        "        logging.warning(\"Image root %s does not exist\", root)\n",
        "        return pd.DataFrame(columns=[\"path\", \"label\"])\n",
        "\n",
        "    if label_map is None:\n",
        "        label_map = {\n",
        "            child.name: child.name\n",
        "            for child in root.iterdir()\n",
        "            if child.is_dir()\n",
        "        }\n",
        "\n",
        "    for class_dir, label in label_map.items():\n",
        "        folder = root / class_dir\n",
        "        if not folder.exists():\n",
        "            logging.warning(\"Missing sub-directory %s for label %s\", folder, label)\n",
        "            continue\n",
        "        for path in folder.rglob(\"*\"):\n",
        "            if path.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\"}:\n",
        "                records.append({\"path\": path, \"label\": label})\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def load_roboflow_logo_dataframe(root: Optional[Path]) -> pd.DataFrame:\n",
        "    if root is None:\n",
        "        logging.warning(\"Roboflow logo dataset root is not available.\")\n",
        "        return pd.DataFrame(columns=[\"path\", \"label\", \"split\"])\n",
        "    if not root.exists():\n",
        "        logging.warning(\"Roboflow logo dataset root %s does not exist\", root)\n",
        "        return pd.DataFrame(columns=[\"path\", \"label\", \"split\"])\n",
        "\n",
        "    frames: List[pd.DataFrame] = []\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        split_dir = root / split\n",
        "        if not split_dir.exists():\n",
        "            continue\n",
        "        split_df = scan_image_directory(split_dir)\n",
        "        if split_df.empty:\n",
        "            continue\n",
        "        split_df[\"split\"] = split\n",
        "        frames.append(split_df)\n",
        "\n",
        "    if frames:\n",
        "        return pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    fallback_df = scan_image_directory(root)\n",
        "    if fallback_df.empty:\n",
        "        return pd.DataFrame(columns=[\"path\", \"label\", \"split\"])\n",
        "    fallback_df[\"split\"] = \"unspecified\"\n",
        "    return fallback_df\n",
        "\n",
        "\n",
        "logo_df = load_roboflow_logo_dataframe(ROBOFLOW_LOGO_ROOT)\n",
        "print(\"Roboflow halal logo dataset size:\", len(logo_df))\n",
        "\n",
        "PACKAGING_LABEL_MAP = {\n",
        "    \"ingredient\": \"ingredient_panel\",\n",
        "    \"packaging\": \"packaging\",\n",
        "}\n",
        "PACKAGING_ROOT = RAW_DIR / \"product_packaging_images\"\n",
        "if PACKAGING_ROOT.exists():\n",
        "    packaging_df = scan_image_directory(PACKAGING_ROOT, PACKAGING_LABEL_MAP)\n",
        "else:\n",
        "    logging.info(\"Packaging dataset directory %s not found; continuing without packaging data.\", PACKAGING_ROOT)\n",
        "    packaging_df = pd.DataFrame(columns=[\"path\", \"label\"])\n",
        "print(\"Packaging/ingredient dataset size:\", len(packaging_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_rows = [\n",
        "    {\"Dataset\": \"Roboflow Halal Logos\", \"Count\": len(logo_df)},\n",
        "    {\"Dataset\": \"Ingredient CSV Files\", \"Count\": len(INGREDIENT_CSV_PATHS)},\n",
        "]\n",
        "\n",
        "if not packaging_df.empty:\n",
        "    dataset_rows.append({\n",
        "        \"Dataset\": \"Packaging/Ingredient (local upload)\",\n",
        "        \"Count\": len(packaging_df),\n",
        "    })\n",
        "\n",
        "display_df = pd.DataFrame(dataset_rows)\n",
        "\n",
        "display(display_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ingredient Text Classifier (Halal / Haram / Mushbooh)\n",
        "\n",
        "We blend the E-code registry with the Kaggle `food-ingredients-dataset-with-halal-label` resource to create labelled ingredient lists, then train an end-to-end Keras model (`TextVectorization` + CNN) that we export as an `.h5` file for mobile deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RULE_BASED_HARAM_KEYWORDS = {\n",
        "    \"alcohol\",\n",
        "    \"ethanol\",\n",
        "    \"rum\",\n",
        "    \"beer\",\n",
        "    \"wine\",\n",
        "    \"bacon\",\n",
        "    \"ham\",\n",
        "    \"porcine\",\n",
        "    \"pork\",\n",
        "    \"lard\",\n",
        "    \"gelatin\",\n",
        "    \"gelatine\",\n",
        "    \"carmine\",\n",
        "    \"e120\",\n",
        "    \"cochineal\",\n",
        "    \"pepsin\",\n",
        "    \"rennet\",\n",
        "    \"lipase\",\n",
        "}\n",
        "\n",
        "RULE_BASED_HALAL_KEYWORDS = {\n",
        "    \"halal certified\",\n",
        "    \"halal-certified\",\n",
        "    \"zabiha\",\n",
        "    \"halal slaughtered\",\n",
        "}\n",
        "\n",
        "def normalise_text(text: Optional[str]) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s,.;:/%()\\-&]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "ecode_status_map = {\n",
        "    row.e_code_number.upper(): row.halal_status\n",
        "    for row in ecode_df.itertuples()\n",
        "    if isinstance(row.e_code_number, str)\n",
        "}\n",
        "\n",
        "ECODE_PATTERN = re.compile(r\"E\\d{1,4}[A-Z]?\")\n",
        "\n",
        "ingredient_texts: List[str] = []\n",
        "ingredient_labels: List[str] = []\n",
        "ingredient_sources: List[str] = []\n",
        "\n",
        "\n",
        "def derive_label_from_text(raw_text: Optional[str]) -> Optional[Tuple[str, str]]:\n",
        "    normalized = normalise_text(raw_text)\n",
        "    if not normalized:\n",
        "        return None\n",
        "    upper_text = normalized.upper()\n",
        "    codes = {code for code in ECODE_PATTERN.findall(upper_text)}\n",
        "    statuses = [ecode_status_map.get(code) for code in codes if ecode_status_map.get(code)]\n",
        "    if any(keyword in normalized for keyword in RULE_BASED_HARAM_KEYWORDS):\n",
        "        label = \"Haram\"\n",
        "    elif statuses and any(status == \"Haram\" for status in statuses):\n",
        "        label = \"Haram\"\n",
        "    elif statuses and all(status == \"Halal\" for status in statuses):\n",
        "        label = \"Halal\"\n",
        "    elif statuses and any(status == \"Mushbooh\" for status in statuses):\n",
        "        label = \"Mushbooh\"\n",
        "    elif any(keyword in normalized for keyword in RULE_BASED_HALAL_KEYWORDS):\n",
        "        label = \"Halal\"\n",
        "    else:\n",
        "        label = None\n",
        "    if label is None:\n",
        "        return None\n",
        "    return normalized, label\n",
        "\n",
        "TEXT_COLUMN_CANDIDATES = {\n",
        "    \"ingredient\",\n",
        "    \"ingredients\",\n",
        "    \"ingredients_text\",\n",
        "    \"ingredient_text\",\n",
        "    \"raw_ingredients\",\n",
        "    \"text\",\n",
        "    \"description\",\n",
        "}\n",
        "\n",
        "LABEL_COLUMN_CANDIDATES = {\n",
        "    \"halal_label\",\n",
        "    \"halal_status\",\n",
        "    \"label\",\n",
        "    \"status\",\n",
        "    \"classification\",\n",
        "    \"category\",\n",
        "}\n",
        "\n",
        "def normalise_kaggle_label(value: Any) -> Optional[str]:\n",
        "    if value is None or (isinstance(value, float) and math.isnan(value)):\n",
        "        return None\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    if isinstance(value, (int, float)):\n",
        "        if value == 1:\n",
        "            return \"Halal\"\n",
        "        if value == 0:\n",
        "            return \"Haram\"\n",
        "    text_value = str(value).strip().lower()\n",
        "    if not text_value:\n",
        "        return None\n",
        "    canonical = re.sub(r\"[^a-z0-9]+\", \" \", text_value).strip()\n",
        "    mapping = {\n",
        "        \"halal\": \"Halal\",\n",
        "        \"h\": \"Halal\",\n",
        "        \"haram\": \"Haram\",\n",
        "        \"non halal\": \"Haram\",\n",
        "        \"non halal product\": \"Haram\",\n",
        "        \"non-halal\": \"Haram\",\n",
        "        \"mushbooh\": \"Mushbooh\",\n",
        "        \"doubtful\": \"Mushbooh\",\n",
        "    }\n",
        "    return mapping.get(canonical)\n",
        "\n",
        "\n",
        "def extract_text_label_pairs_from_df(df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
        "    available_text_cols = [col for col in df.columns if col.lower() in TEXT_COLUMN_CANDIDATES]\n",
        "    available_label_cols = [col for col in df.columns if col.lower() in LABEL_COLUMN_CANDIDATES]\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    if available_text_cols and available_label_cols:\n",
        "        text_col = available_text_cols[0]\n",
        "        label_col = available_label_cols[0]\n",
        "        for raw_text, raw_label in df[[text_col, label_col]].dropna().values:\n",
        "            normalized_text = normalise_text(raw_text)\n",
        "            normalized_label = normalise_kaggle_label(raw_label)\n",
        "            if normalized_text and normalized_label:\n",
        "                pairs.append((normalized_text, normalized_label))\n",
        "\n",
        "    if not pairs and available_text_cols:\n",
        "        logging.info(\"Falling back to heuristic labelling for Kaggle ingredient dataset.\")\n",
        "        for text_col in available_text_cols:\n",
        "            for raw_text in df[text_col].dropna().astype(str).tolist():\n",
        "                derived = derive_label_from_text(raw_text)\n",
        "                if derived:\n",
        "                    pairs.append(derived)\n",
        "    return pairs\n",
        "\n",
        "\n",
        "if INGREDIENT_CSV_PATHS:\n",
        "    for csv_path in INGREDIENT_CSV_PATHS:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception as exc:\n",
        "            logging.warning(\"Failed to load %s: %s\", csv_path, exc)\n",
        "            continue\n",
        "        pairs = extract_text_label_pairs_from_df(df)\n",
        "        if not pairs:\n",
        "            logging.warning(\"No labelled records extracted from %s\", csv_path)\n",
        "            continue\n",
        "        for normalized, label in pairs:\n",
        "            ingredient_texts.append(normalized)\n",
        "            ingredient_labels.append(label)\n",
        "            ingredient_sources.append(csv_path.stem)\n",
        "else:\n",
        "    logging.warning(\"No ingredient CSV files available. Only E-code data will be used for training.\")\n",
        "\n",
        "# Augment with E-code descriptive texts to balance classes\n",
        "for row in ecode_df.itertuples():\n",
        "    if row.halal_status == \"Unknown\":\n",
        "        continue\n",
        "    combined_text = f\"{row.e_code_number or ''} {row.name or ''} {row.description or ''}\"\n",
        "    normalized, label = normalise_text(combined_text), row.halal_status\n",
        "    if normalized:\n",
        "        ingredient_texts.append(normalized)\n",
        "        ingredient_labels.append(label)\n",
        "        ingredient_sources.append(\"ecode\")\n",
        "\n",
        "ingredient_training_df = pd.DataFrame({\n",
        "    \"text\": ingredient_texts,\n",
        "    \"label\": ingredient_labels,\n",
        "    \"source\": ingredient_sources,\n",
        "}).drop_duplicates(\"text\").reset_index(drop=True)\n",
        "\n",
        "print(\"Ingredient training samples:\", len(ingredient_training_df))\n",
        "print(ingredient_training_df[\"label\"].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_TOKENS = 20000\n",
        "SEQUENCE_LENGTH = 200\n",
        "TEXT_BATCH_SIZE = 64\n",
        "\n",
        "texts = ingredient_training_df[\"text\"].to_numpy()\n",
        "labels = ingredient_training_df[\"label\"].to_numpy()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts,\n",
        "    encoded_labels,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=encoded_labels,\n",
        ")\n",
        "\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        "    standardize=None,\n",
        ")\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices(X_train).batch(TEXT_BATCH_SIZE))\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "def make_text_dataset(text_array, label_array, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((text_array, label_array))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(len(text_array), seed=SEED)\n",
        "    return ds.batch(TEXT_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_text_dataset(X_train, y_train, shuffle=True)\n",
        "val_ds = make_text_dataset(X_test, y_test, shuffle=False)\n",
        "\n",
        "ingredient_model = keras.Sequential([\n",
        "    vectorizer,\n",
        "    layers.Embedding(MAX_TOKENS + 1, 128, mask_zero=True),\n",
        "    layers.Conv1D(128, 5, activation=\"relu\"),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "ingredient_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "ingredient_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2, min_lr=1e-5),\n",
        "]\n",
        "\n",
        "history = ingredient_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=12,\n",
        "    callbacks=ingredient_callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "val_probs = ingredient_model.predict(val_ds, verbose=0)\n",
        "y_pred = val_probs.argmax(axis=1)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Ingredient Text Classifier Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INGREDIENT_MODEL_PATH = MODELS_DIR / \"ingredient_text_classifier.h5\"\n",
        "INGREDIENT_LABELS_PATH = MODELS_DIR / \"ingredient_text_labels.json\"\n",
        "\n",
        "ingredient_model.save(INGREDIENT_MODEL_PATH)\n",
        "with open(INGREDIENT_LABELS_PATH, \"w\", encoding=\"utf-8\") as fp:\n",
        "    json.dump(label_encoder.classes_.tolist(), fp)\n",
        "\n",
        "print(\"Saved ingredient classifier to\", INGREDIENT_MODEL_PATH)\n",
        "print(\"Saved ingredient label map to\", INGREDIENT_LABELS_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Barcode Status Classifier (Halal / Haram)\n",
        "\n",
        "We reuse the scraped E-code registry as a proxy for additive barcodes. Each codeâ€™s numeric signature becomes a feature vector, and we train a multinomial logistic regression model that predicts `Halal`, `Haram`, or `Mushbooh`. The resulting pipeline can score barcodes directly and is saved for backend integration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "barcode_df = ecode_df.copy()\n",
        "barcode_df[\"barcode\"] = barcode_df[\"e_code_number\"].str.replace(\"E\", \"\", regex=False)\n",
        "barcode_df = barcode_df[barcode_df[\"barcode\"].str.isdigit()]\n",
        "barcode_df = barcode_df[barcode_df[\"halal_status\"] != \"Unknown\"].reset_index(drop=True)\n",
        "\n",
        "if not barcode_df.empty:\n",
        "    barcode_texts = barcode_df[\"barcode\"].astype(str).to_numpy()\n",
        "    barcode_labels = barcode_df[\"halal_status\"].to_numpy()\n",
        "\n",
        "    barcode_label_encoder = LabelEncoder()\n",
        "    encoded_barcode_labels = barcode_label_encoder.fit_transform(barcode_labels)\n",
        "\n",
        "    Xb_train, Xb_test, yb_train, yb_test = train_test_split(\n",
        "        barcode_texts,\n",
        "        encoded_barcode_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "        stratify=encoded_barcode_labels,\n",
        "    )\n",
        "\n",
        "    BARCODE_MAX_LENGTH = max(len(code) for code in barcode_texts)\n",
        "\n",
        "    barcode_vectorizer = TextVectorization(\n",
        "        max_tokens=16,\n",
        "        standardize=None,\n",
        "        split=\"character\",\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=BARCODE_MAX_LENGTH,\n",
        "    )\n",
        "    barcode_vectorizer.adapt(tf.data.Dataset.from_tensor_slices(Xb_train))\n",
        "\n",
        "    BARCODE_BATCH = 32\n",
        "\n",
        "    def make_barcode_ds(text_array, label_array, shuffle=True):\n",
        "        ds = tf.data.Dataset.from_tensor_slices((text_array, label_array))\n",
        "        if shuffle:\n",
        "            ds = ds.shuffle(len(text_array), seed=SEED)\n",
        "        return ds.batch(BARCODE_BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    barcode_train_ds = make_barcode_ds(Xb_train, yb_train, shuffle=True)\n",
        "    barcode_val_ds = make_barcode_ds(Xb_test, yb_test, shuffle=False)\n",
        "\n",
        "    barcode_num_classes = len(barcode_label_encoder.classes_)\n",
        "\n",
        "    barcode_model = keras.Sequential([\n",
        "        barcode_vectorizer,\n",
        "        layers.Embedding(barcode_vectorizer.vocabulary_size(), 32, mask_zero=True),\n",
        "        layers.Bidirectional(layers.LSTM(32)),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(barcode_num_classes, activation=\"softmax\"),\n",
        "    ])\n",
        "\n",
        "    barcode_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=5e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    barcode_callbacks = [\n",
        "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    ]\n",
        "\n",
        "    barcode_model.fit(\n",
        "        barcode_train_ds,\n",
        "        validation_data=barcode_val_ds,\n",
        "        epochs=20,\n",
        "        callbacks=barcode_callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    barcode_probs = barcode_model.predict(barcode_val_ds, verbose=0)\n",
        "    barcode_predictions = barcode_probs.argmax(axis=1)\n",
        "    print(classification_report(yb_test, barcode_predictions, target_names=barcode_label_encoder.classes_))\n",
        "\n",
        "    BARCODE_MODEL_PATH = MODELS_DIR / \"barcode_status_classifier.h5\"\n",
        "    BARCODE_LABELS_PATH = MODELS_DIR / \"barcode_status_labels.json\"\n",
        "\n",
        "    barcode_model.save(BARCODE_MODEL_PATH)\n",
        "    with open(BARCODE_LABELS_PATH, \"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(barcode_label_encoder.classes_.tolist(), fp)\n",
        "\n",
        "    print(\"Saved barcode classifier to\", BARCODE_MODEL_PATH)\n",
        "    print(\"Saved barcode label map to\", BARCODE_LABELS_PATH)\n",
        "else:\n",
        "    print(\"No numeric E-code data available to train barcode classifier.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Halal Logo Detector (Transfer Learning)\n",
        "\n",
        "We fine-tune a MobileNetV2 backbone for binary classification (`logo` vs `no_logo`). Training automatically skips if the dataset is missing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def build_logo_datasets(df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        logging.warning(\"Logo dataset empty; skipping training\")\n",
        "        return None\n",
        "\n",
        "    temp_manifest = PROCESSED_DIR / \"logo_manifest.csv\"\n",
        "    df.to_csv(temp_manifest, index=False)\n",
        "\n",
        "    dataset = (\n",
        "        tf.data.Dataset.from_tensor_slices((df[\"path\"].astype(str).tolist(), df[\"label\"].tolist()))\n",
        "        .shuffle(len(df), seed=SEED)\n",
        "    )\n",
        "\n",
        "    def load_image(path, label):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_image(image, channels=3)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        image = tf.image.resize(image, IMG_SIZE)\n",
        "        return image, label\n",
        "\n",
        "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    class_names = sorted(df[\"label\"].unique())\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(df[\"label\"])\n",
        "\n",
        "    train_df, val_df, train_labels, val_labels = train_test_split(\n",
        "        df[\"path\"].astype(str),\n",
        "        encoded_labels,\n",
        "        test_size=0.2,\n",
        "        stratify=encoded_labels,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    def make_dataset(paths, labels):\n",
        "        ds = tf.data.Dataset.from_tensor_slices((paths.tolist(), labels.tolist()))\n",
        "        ds = ds.shuffle(len(paths), seed=SEED)\n",
        "        ds = ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    train_ds = make_dataset(train_df, train_labels)\n",
        "    val_ds = make_dataset(val_df, val_labels)\n",
        "\n",
        "    return train_ds, val_ds, label_encoder\n",
        "\n",
        "logo_data = build_logo_datasets(logo_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOGO_MODEL_PATH = MODELS_DIR / \"halal_logo_detector.keras\"\n",
        "LOGO_MODEL_H5_PATH = MODELS_DIR / \"halal_logo_detector.h5\"\n",
        "\n",
        "if LOGO_MODEL_PATH.exists() and LOGO_MODEL_H5_PATH.exists():\n",
        "    print(\"Logo detector artefacts available:\", LOGO_MODEL_PATH.name, LOGO_MODEL_H5_PATH.name)\n",
        "else:\n",
        "    print(\"Logo detector checkpoint not found; training will start now.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if logo_data is None:\n",
        "    logo_model = None\n",
        "else:\n",
        "    train_ds, val_ds, logo_label_encoder = logo_data\n",
        "\n",
        "    base_model = keras.applications.MobileNetV2(\n",
        "        input_shape=(*IMG_SIZE, 3), include_top=False, weights=\"imagenet\"\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
        "    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    logo_model = keras.Model(inputs, outputs)\n",
        "    logo_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2, min_lr=1e-5),\n",
        "    ]\n",
        "\n",
        "    history = logo_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=10,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    hist_df.to_csv(OUTPUTS_DIR / \"logo_training_history.csv\", index=False)\n",
        "    dump(logo_label_encoder, MODELS_DIR / \"logo_label_encoder.joblib\")\n",
        "\n",
        "    logo_model.save(LOGO_MODEL_PATH)\n",
        "    logo_model.save(LOGO_MODEL_H5_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LOGO_MODEL_PATH.exists():\n",
        "    logo_model = keras.models.load_model(LOGO_MODEL_PATH)\n",
        "    print(\"Loaded logo model from\", LOGO_MODEL_PATH)\n",
        "else:\n",
        "    logo_model = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Unified Inference Helpers\n",
        "\n",
        "We expose reusable functions to:\n",
        "\n",
        "- Run OCR (EasyOCR) and clean ingredient text.\n",
        "- Classify ingredients via the trained text model.\n",
        "- Detect halal logos via the CNN.\n",
        "- Decode barcodes via `pyzbar`.\n",
        "- Combine rule-based logic with learned predictions to produce a final verdict.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = easyocr.Reader([\"en\"], gpu=torch.cuda.is_available())\n",
        "\n",
        "ingredient_classifier = None\n",
        "ingredient_label_classes: List[str] = []\n",
        "if INGREDIENT_MODEL_PATH.exists():\n",
        "    ingredient_classifier = keras.models.load_model(INGREDIENT_MODEL_PATH)\n",
        "    with open(INGREDIENT_LABELS_PATH, \"r\", encoding=\"utf-8\") as fp:\n",
        "        ingredient_label_classes = json.load(fp)\n",
        "\n",
        "barcode_classifier = None\n",
        "barcode_label_classes: List[str] = []\n",
        "BARCODE_MODEL_PATH = MODELS_DIR / \"barcode_status_classifier.h5\"\n",
        "BARCODE_LABELS_PATH = MODELS_DIR / \"barcode_status_labels.json\"\n",
        "if BARCODE_MODEL_PATH.exists() and BARCODE_LABELS_PATH.exists():\n",
        "    barcode_classifier = keras.models.load_model(BARCODE_MODEL_PATH)\n",
        "    with open(BARCODE_LABELS_PATH, \"r\", encoding=\"utf-8\") as fp:\n",
        "        barcode_label_classes = json.load(fp)\n",
        "\n",
        "\n",
        "def classify_text(text: str) -> Dict[str, float]:\n",
        "    if ingredient_classifier is None or not ingredient_label_classes:\n",
        "        return {label: 0.0 for label in [\"Halal\", \"Haram\", \"Mushbooh\"]}\n",
        "    normalized = normalise_text(text)\n",
        "    if not normalized:\n",
        "        return {label: 0.0 for label in ingredient_label_classes}\n",
        "    probs = ingredient_classifier.predict([normalized], verbose=0)[0]\n",
        "    return {\n",
        "        ingredient_label_classes[idx]: float(score)\n",
        "        for idx, score in enumerate(probs)\n",
        "    }\n",
        "\n",
        "\n",
        "def classify_barcode_value(barcode: str) -> Optional[Tuple[str, float, Dict[str, float]]]:\n",
        "    if barcode_classifier is None or not barcode_label_classes:\n",
        "        return None\n",
        "    probs = barcode_classifier.predict([barcode], verbose=0)[0]\n",
        "    best_idx = int(np.argmax(probs))\n",
        "    distribution = {\n",
        "        barcode_label_classes[idx]: float(score)\n",
        "        for idx, score in enumerate(probs)\n",
        "    }\n",
        "    return barcode_label_classes[best_idx], float(probs[best_idx]), distribution\n",
        "\n",
        "\n",
        "def rule_based_status(text: str) -> Optional[str]:\n",
        "    normalized = normalise_text(text)\n",
        "    for keyword in RULE_BASED_HARAM_KEYWORDS:\n",
        "        if keyword in normalized:\n",
        "            return \"Haram\"\n",
        "    for keyword in RULE_BASED_HALAL_KEYWORDS:\n",
        "        if keyword in normalized:\n",
        "            return \"Halal\"\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloadable Artefacts\n",
        "\n",
        "- `barcode_status_classifier.h5`\n",
        "- `barcode_status_labels.json`\n",
        "- `ingredient_text_classifier.h5`\n",
        "- `ingredient_text_labels.json`\n",
        "- `halal_logo_detector.keras`\n",
        "- `halal_logo_detector.h5`\n",
        "- `logo_label_encoder.joblib`\n",
        "\n",
        "Use `from google.colab import files; files.download(path)` to pull each model to your machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_barcodes(image_path: Path) -> List[str]:\n",
        "    image = cv2.imread(str(image_path))\n",
        "    if image is None:\n",
        "        return []\n",
        "    barcodes = pyzbar.decode(image)\n",
        "    return [barcode.data.decode(\"utf-8\") for barcode in barcodes]\n",
        "\n",
        "\n",
        "def detect_logo(image_path: Path) -> Tuple[bool, float]:\n",
        "    if logo_model is None:\n",
        "        return False, 0.0\n",
        "    image = tf.io.read_file(str(image_path))\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "    preds = logo_model.predict(tf.expand_dims(image, axis=0), verbose=0)[0][0]\n",
        "    return preds >= 0.5, float(preds)\n",
        "\n",
        "\n",
        "def run_ocr(image_path: Path) -> str:\n",
        "    result = reader.readtext(str(image_path), detail=0, paragraph=True)\n",
        "    text = \" \\n\".join(result)\n",
        "    return normalise_text(text)\n",
        "\n",
        "\n",
        "def classify_product_image(\n",
        "    image_path: Path,\n",
        "    product_name: str = \"Unknown Product\",\n",
        "    barcode_hint: Optional[str] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    if not image_path.exists():\n",
        "        raise FileNotFoundError(image_path)\n",
        "\n",
        "    barcode_values = decode_barcodes(image_path)\n",
        "    if barcode_hint:\n",
        "        barcode_values.append(barcode_hint)\n",
        "    barcode_values = list(dict.fromkeys(barcode_values))  # deduplicate preserving order\n",
        "\n",
        "    ocr_text = run_ocr(image_path)\n",
        "    text_probs = classify_text(ocr_text)\n",
        "    rule_status = rule_based_status(ocr_text)\n",
        "    logo_flag, logo_conf = detect_logo(image_path)\n",
        "\n",
        "    barcode_assessments = []\n",
        "    for code in barcode_values:\n",
        "        barcode_result = classify_barcode_value(code)\n",
        "        if barcode_result:\n",
        "            status, conf, distribution = barcode_result\n",
        "            barcode_assessments.append({\n",
        "                \"barcode\": code,\n",
        "                \"status\": status,\n",
        "                \"confidence\": conf,\n",
        "                \"distribution\": distribution,\n",
        "            })\n",
        "\n",
        "    ecode_matches = sorted({match for match in ECODE_PATTERN.findall(ocr_text.upper())})\n",
        "    ecode_assessments = [\n",
        "        {\n",
        "            \"code\": code,\n",
        "            \"status\": ecode_status_map.get(code, \"Unknown\"),\n",
        "        }\n",
        "        for code in ecode_matches\n",
        "    ]\n",
        "\n",
        "    ingredient_status = max(text_probs, key=text_probs.get) if text_probs else \"Unknown\"\n",
        "    ingredient_confidence = float(text_probs.get(ingredient_status, 0.0))\n",
        "\n",
        "    weight_map = {\"Halal\": 0.0, \"Haram\": 0.0, \"Mushbooh\": 0.0}\n",
        "\n",
        "    for assessment in barcode_assessments:\n",
        "        status = assessment[\"status\"]\n",
        "        weight_map[status] = weight_map.get(status, 0.0) + assessment[\"confidence\"]\n",
        "\n",
        "    if rule_status:\n",
        "        weight_map[rule_status] = weight_map.get(rule_status, 0.0) + 1.0\n",
        "\n",
        "    weight_map[ingredient_status] = weight_map.get(ingredient_status, 0.0) + ingredient_confidence\n",
        "\n",
        "    for assessment in ecode_assessments:\n",
        "        status = assessment[\"status\"]\n",
        "        weight_map[status] = weight_map.get(status, 0.0) + 0.6\n",
        "\n",
        "    if logo_flag:\n",
        "        weight_map[\"Halal\"] = weight_map.get(\"Halal\", 0.0) + logo_conf\n",
        "\n",
        "    verdict_priority = [\"Haram\", \"Halal\", \"Mushbooh\"]\n",
        "\n",
        "    def priority_score(status: str) -> float:\n",
        "        if status in verdict_priority:\n",
        "            return -verdict_priority.index(status)\n",
        "        return -float(len(verdict_priority))\n",
        "\n",
        "    if any(weight_map.values()):\n",
        "        top_status = max(weight_map.items(), key=lambda item: (item[1], priority_score(item[0])))[0]\n",
        "    else:\n",
        "        top_status = \"Unknown\"\n",
        "\n",
        "    ingredient_analysis = {\n",
        "        \"predicted_status\": ingredient_status,\n",
        "        \"confidence\": ingredient_confidence,\n",
        "        \"probabilities\": text_probs,\n",
        "        \"rule_based\": rule_status,\n",
        "    }\n",
        "\n",
        "    logo_analysis = {\n",
        "        \"detected\": logo_flag,\n",
        "        \"confidence\": logo_conf,\n",
        "    }\n",
        "\n",
        "    ecode_analysis_str = \", \".join(\n",
        "        f\"{entry['code']} ({entry['status']})\" for entry in ecode_assessments\n",
        "    ) if ecode_assessments else \"No E-codes detected\"\n",
        "\n",
        "    return {\n",
        "        \"Product Name\": product_name,\n",
        "        \"Barcode\": barcode_values[0] if barcode_values else None,\n",
        "        \"E-Code Analysis\": {\n",
        "            \"summary\": ecode_analysis_str,\n",
        "            \"details\": ecode_assessments,\n",
        "        },\n",
        "        \"Ingredient Analysis\": ingredient_analysis,\n",
        "        \"Logo Detection\": logo_analysis,\n",
        "        \"Barcode Analysis\": barcode_assessments,\n",
        "        \"Final Verdict\": top_status,\n",
        "        \"Evidence\": {\n",
        "            \"weighting\": weight_map,\n",
        "            \"ocr_text\": ocr_text,\n",
        "        },\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: update `sample_image_path` to point at any product photo (packaging / ingredient panel)\n",
        "sample_image_path = RAW_DIR / \"demos\" / \"sample_product.jpg\"\n",
        "\n",
        "if sample_image_path.exists():\n",
        "    result = classify_product_image(sample_image_path)\n",
        "    print(json.dumps(result, indent=2))\n",
        "else:\n",
        "    logging.warning(\"Sample image not found at %s. Add a demo image to run the inference showcase.\", sample_image_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next Steps & Export\n",
        "\n",
        "- ðŸ“ Trained artefacts live under `MODELS_DIR` and can be downloaded from Colab with `files.download(...)`.\n",
        "- ðŸ” To retrain with new data, refresh the raw folders and rerun Sections 2â€“3.\n",
        "- ðŸš€ Integrate with your FastAPI backend by loading `ingredient_text_classifier.joblib` and `halal_logo_detector.keras`, then reusing the helper logic above (OCR, barcode, rules).\n",
        "\n",
        "Happy shipping! ðŸŽ‰\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
